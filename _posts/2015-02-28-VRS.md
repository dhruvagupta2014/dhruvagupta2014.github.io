---
layout: post
title: A Visual Recommender System using Deep Learning
subtitle: A system to intelligently recommend similar products. 
gh-repo: dhruvagupta2014/Visual-Recommender-System
gh-badge: [star, fork, follow]
tags: [test]
comments: true
---

Deep Learning is making waves in a lot of areas today. A lot of industry use cases are getting disrupted in the sense that traditional approaches are being replaced by Machine Learning/Deep Learning approaches. I think a major catalyst is the concept of Open Source. Deep Learning libraries like Tensorflow, Caffe, Pytorch etc have given the reigns of a highly academic field to Software developers. 

![Deep Learning Libraries](s3://dhruvagupta28jan2020bucket/Github.io/VRS/VRS_1.png)


In this post I’ll show you how you can leverage the power of such open source tools to make a Visual Recommender System. There are multiple use cases for this, but just to take an example, let’s say you went out for shopping with friends and loved a blue dress which is priced way over what you want to pay for. Well, you can go online and search for dresses similar to this blue dress which is within your budget using the Visual Recommender System(VRS). We’re going to utilize a popular concept called Transfer Learning to build the VRS . So, what is Transfer Learning ?

![Deep Learning Libraries](s3://dhruvagupta28jan2020bucket/Github.io/VRS/VRS2.png)


In short, and in the context of images, a Deep Learning model is a layered architecture and can be designed to perform various tasks on a dataset. For eg. Classifying which animal an image contains. (this task is also called ‘classification’). There are other tasks possible, like speech recognition, stock market predictions etc. 

#![Deep Learning Libraries](https://github.com/dhruvagupta2014/dhruvagupta2014.github.io/blob/master/img/VRS_1.png)

Different layers learn different things about the data. For eg. If your data is a set of images of animals, the initial few layers might learn to identify very low level features of the images, for eg. the colors, edges, contours (note that these features are general to any kind of image) etc while the intermediate layers might learn to identify features more specific to our data (animals in our case) like whether there’s a tail or not, or whether the current image has an animal which is 4 legged like a lion or two legged like a kangaroo etc. The final layer can tell us which animal it is (if the deep learning model is designed to perform classification).

#![Deep Learning Libraries](https://github.com/dhruvagupta2014/dhruvagupta2014.github.io/blob/master/img/VRS_1.png)

Note that the above Deep Learning model was only trained on animals so even the low level features like colors or edges might be specific to animals. But if your model is trained on a huge dataset of millions of images from everyday life like cars, fruits, highways, people etc. then the initial few layers would learn to extract these generic low level building blocks like edges, contours, colors, shapes which remain the same for any kind of image (since all images are made of more or less the same basic building blocks). These low level features learned by the thus trained model can be re-used so as to save valuable time and also lets the model train by relatively smaller dataset.  This is the basis of transfer learning. 

More specifically, here is the procedure for Transfer Learning , 
1.	Download/import a pre-trained model (easily available online)
2.	Keep the learnings of initial layers (features) and discard the learnings of last few layers (task specific) 
3.	Use the features extracted to perform a specific task (like classification, or visual recommendation etc.)

Let’s take a deeper look at these steps :

1.	We’e can use a popular pre-trained model called Inception_v3 which has been trained on Imagenet dataset (A dataset consisting of thousands of categories of objects and millions of images) Inception V3 extracts the features (also called as bottleneck features) of an image.

2.	For every image, a feature vector file can be generated which for inception_v3 model is an array of 2048 values (think of it as 2048 different features)

3.	Now that we have the image represented as feature vectors, all we have to do extract images similar to a target image is calculate similarity between the features of our target image and the features of all other images. We can do this by calculating distance (we’ll be using eucledian distance here) between target feature vector (let’s call it feature_target) and all the other images in our dataset (let’s call them feature_image1, feature_image2 etc.)

Suppose we have to get 2 closest images from 1000 images, then we can take the images corresponding to the smallest 2 distances. In above case, we can display image1000 and image2 since they have the least distances. 



#![Deep Learning Libraries](https://github.com/dhruvagupta2014/dhruvagupta2014.github.io/blob/master/img/VRS_1.png)







## Boxes
You can add notification, warning and error boxes like this:

### Notification

{: .box-note}
**Note:** This is a notification box.

### Warning

{: .box-warning}
**Warning:** This is a warning box.

### Error

{: .box-error}
**Error:** This is an error box.
